---
title: "Intro to nimbleEcology: Marginalization and the occupancy model"
description: |
  An introductory guide to estimating site-occupancy models in a Bayesian or maximum likelihood framework.
author:
  - name: Ben R. Goldstein
date: 2022-02-22
output:
  distill::distill_article:
    self_contained: false
    toc: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(digits = 2)
```

# In this post...

I introduce the concept of "marginalization" in a hierarchical model
through the context of the occupancy model.

I assume a basic familiarity with the occupancy model, which I define in
the section "Overview of the site-occupancy model" below, and a basic
familiarity with / exposure to Bayesian model estimation (MCMC). Neither
of these is strictly necessary to get the main points, but two good
places to get that background could be here and here.

I focus on an R package I co-authored, `nimbleEcology`, in which we've
implemented some marginalized distributions for common ecological
models.

# What is nimbleEcology?

nimbleEcology is an R package that makes it easier to implement some
common ecological models in the general statistical software NIMBLE.
Specifically, nimbleEcology provides marginalized distributions that
turn hierarchical structures with latent states, such as the occupancy
or N-mixture model, into one line distributions similar to familiar
distributions like `dpois()` or `dbinom()`.

In this blog post, I show how the entire occupancy hierarchical
probability distribution for the occupancy model can be written with
`y ~ dOcc_v(...)` and provide a worked example.

### What is marginalization?

Model fitting, whether working with Bayesian or maximum likelihood
tools, always involves calculating the likelihood of a model. In MLE, we
often calculate the likelihood of a set of parameters given our data and
numerically optimize those parameters to estimate the maximum of the
likelihood function (hence "maximum likelihood estimation"). This is how
popular MLE packages for ecological models like `unmarked` work under
the hood. In Bayesian statistics, we don't directly optimize the
likelihood, but we do calculate it for a set of parameters when deciding
whether to accept a newly sampled value during MCMC sampling.

When we have latent states and random effects in our model, calculating
the likelihood becomes more complicated. We don't include values for
latent states in the total model likelihood, because by definition
random effects and latent states are not technically parameters we're
estimating--that's what differentiates them from fixed effects. Instead
we want to know the likelihood of our parameters, given the data and a
random effect/latent state structure with specific random effects/latent
states *that could have any value* in their domain. To optimize the
likelihood without optimizing specific random effect values, you have to
calculate and integrate over all possible values of each random effect.

(**A brief note here on latent states**. I'm honestly not sure what the
definition of "latent state" is. In practice in ecology, a latent state
is a piece of model structure that represents an unobserved part of the
system, usually as part of a hierarchical structure. I believe you can
usually think of latent states as a type of random effect. Latent states
come up in species distribution modeling as underlying occupancy
conditions or abundance conditions. They can also come up in multi-state
models of animal movement or survival.)

The rapid expansion of Bayesian methods in ecology has both obscured and
been spurred by the fact that writing explicit likelihoods that
integrate over random effects / marginalize over latent states is often
impossible. In Bayesian statistics, you can sample the random effect
without having to marginalize over it due to ~~magic~~ math. This is
nice in cases like continuous random effects where integrating would
require complicated approximators, and also in cases like hierarchical
modeling where it's intuitive to see the model structure written out.

But if we want to use maximum likelihood, and for some Bayesian
applications, it is sometimes possible to marginalize over or integrate
over a latent state or random effect by hand. This means writing a
likelihood function that considers all possible latent states and
computes the likelihood as a weighted sum across those.

This idea still gives me a lot of trouble, so if you're stuck on it,
that's okay--I'll try to contextualize the effects and advantages of
marginalization in a way that doesn't rely on a nuanced statistical
understanding. In a later section, we'll look at the specific case of
marginalizing the occupancy distribution as a specific example.

### Why should I use nimbleEcology marginalized distributions?

Some advantages:

-   **Simpler code**. We went ahead and implemented the occupancy
    distribution so you don't have to. This saves lines of code and
    hopefully reduces some of the headache that inevitably comes from
    debugging a new model.
-   **It's easily modifiable**. Implementing models in nimbleEcology,
    compared to something like unmarked, makes it easier to modify and
    customize your model when you're ready to get fancy.
-   **It's fairly efficient**. nimbleEcology uses marginalized
    distributions, saving memory and sometimes (but not always)
    increasing computational efficiency.
-   **Swap between MLE or Bayesian methodology**. This maybe isn't a
    *need* so much as an interesting feature, but with NIMBLE you have
    access to both maximum likelihood and Bayesian (MCMC) estimation
    with very few changes to the model code.

Some important disadvantages of using marginalized distributions:

-   **No access to posterior distributions of latent states.** It's
    fairly common now for ecologists to create derived products by
    summarizing latent states, such as obtaining a posterior on the
    total number of occupied sites. Because we aren't sampling the
    latent state if we marginalize, we can't do this.

-   **It's sometimes less efficient for MCMC.** The paper that generated
    much of the code that eventually became nimbleEcology is Ponisio et
    al. 2020, "One size does not fit all: Customizing MCMC methods for
    hierarchical models using NIMBLE." The title does a great job of
    summarizing their findings: Dr. Ponisio and colleagues found that
    marginalizing over latent states sometimes, but not always,
    improved MCMC.

# Overview of the site-occupancy model

Much has been written about the occupancy model, including entire
textbooks and [other very nice blog
posts](https://masonfidino.com/a_simple_static_occupancy_model/). I link
to some of those at the bottom of the article. But here's a very brief
overview of this model's structure.

***If you feel comfortable implementing occupancy models, you can skip
ahead to the next section: "Implementing the occupancy model with
nimbleEcology"***

In species distribution modeling, we often want to ask whether a
species' presence on the landscape is correlated with a spatio-temporal
variable. For example, we might be interested in whether a species
prefers dense or sparse tree cover, whether a species' range is growing
or declining over time, or whether a management strategy is actually
working to attract a species.

To answer the question, we go out and collect data. We visit a gradient
of sites across the variable of interest (such as some densely forested
sites and some sparsely forested sites) and write down when we do and
don't detect the animal. We then often want to use statistical modeling
to differentiate between the occupancy and detection processes--i.e.,
whether we detected the species less because it was truly there less, or
just because it was harder to detect.

Site-occupancy models are a type of statistical structure that allow us
to ask these questions when our detection process is imperfect, so we
can't be confident that a nondetection means the species wasn't there.

### Structure

Let's start by putting down some statistical equations for the simplest
hierarchical site-occupancy model.

$$y_{ij} \sim \text{Bernoulli}(z_i \times p_{ij})$$
$$z_i \sim \text{Bernoulli}(\psi_i)$$
$$\text{logit}(\psi_{i}) = x_{i}^T\beta$$
$$\text{logit}(p_{ij}) = w_{ij}^T\gamma$$

where $i$ and $j$ index site and replicate visit, respectively; $y_{ij}$
is a single detection or nondetection (1 or 0) at site $i$, replicate
$j$; $z_i$ is a latent state representing occupancy; $\psi_i$ is the
probability that site $i$ is occupied; $p_{ij}$ is an
observation-specific conditional detection probability; $x_i$ and
$w_{ij}$ are vectors of covariates of interest; and $\beta$ and $\gamma$
are vectors of coefficients representing the effect of covariates on
occupancy and detection, respectively.

Phew. Let's break that down.

In the **first equation**, we describe the probability distribution for
our detection-nondetection observations, $y_{ij}$. We say that each data
point is Bernoulli-distributed: we have a probability of
$z_i \times p_{ij}$ of seeing the animal. Since we either see it or we
don't, the probability of not seeing the animal is
$1 - z_i \times p_{ij}$.

In the occupancy model, we assume that each "site" is in fact occupied
or unoccupied, but that we can't observe this condition perfectly. (I
put "site" in quotation marks because the "site" need not be strictly
spatial--for example, we could treat the occupancy status of a single
location in different years as independent. Because the assumption that
occupancy status doesn't change is called "closure," we can also call
this unit a "unit of closure.") In our **second equation**, we represent
this with the latent state $z_i$, which has a value of 1 if site $i$ is
occupied and 0 if it is not. If the site is unoccupied, we have no
chance of seeing the animal. In that case, $z_i = 0$ and the probability
of $y_ij = 1$ is also 0. Think of $z_i$ as a switch that turns the
probability of seeing the animal on or off. The "conditional probability
of detection," i.e. the probability that we see the animal when it's
present on a given observation, is $p_ij$. It's conditional because it
only affects the data likelihood when $z_i =1$. In the second equation,
we define the distribution for the latent state $z_i$, which has a
probability of $\psi_i$.

The **third equation** links our probability of occupancy, $\psi_i$, to
covariates. We say that, on the logit scale (which is a popular but
largely arbitrary transformation from $(-\infty, \infty)$ to $(0, 1)$),
$\psi_i$ is a linear combination of some observed data describing our
site, $x_{i}$, with coefficients $\beta$. We want to estimate $\beta$ to
get confidence or credible intervals on those effects.

The **fourth equation** replicates this logit-link structure for our
detection probability, $p$, and some covariates $w$ with coefficients
$\gamma$. The detection probability gives the Bernoulli probability that
you observe the animal in question on a visit to an occupied site, i.e.
the probability that $y_{ij}=1$ given that $z_i = 1$.

# A marginalized occupancy distribution, `dOcc`

Ignoring covariates, the non-marginalized occupancy distribution is
written (in NIMBLE code) as

```{r eval = FALSE}
nimbleCode({
  for (i in 1:nsite) {
    z[i] ~ dbern(psi)
    for (j in 1:nrep) {
      y[i, j] ~ dbern(z[i] * p)
    }
  }
})
```

In this setup we've coded a latent state, `z`, and defined it as
following a Bernoulli distribution. Then we've said that our data, `y`,
follow another Bernoulli distribution incorporating the state `z`.

I want to marginalize over $z_i$ and completely eliminate it from my
model. To do this, I consider that the Bernoulli distribution means $z$
can actually only have two states: either $z_i=1$ with probability
$\psi$ or $z_i=0$ with probability $1-\psi$ .

Then, I notice that, if we know what $z_i$ is, the probability of
$y_{ij}$ is easy

But, recall that $z_i$ influences the likelihood of every $y_{ij}$. This
means that our likelihood function needs to take into account all the
observations in $y_{ij}$ simultaneously, because the probability is
interdependent between them. For example, the likelihood of $\psi_i=0$
will be exactly 0 for *all* observations $y_{ij}$ if *any* of them are
nonzero, since an unoccupied site can't produce nonzero observations
under this model.

Thus, the marginalized likelihood of parameters given the observations
at site i is

$$L(\psi,p|y_{i.}) = \psi p^k (1-p)^{n-k} + (1-\psi)I(k = 0)$$

where $n$ is the number of observations at site $i$, $k$ is the number
of **nonzero** observations at site $i$, and $I(k=0)$ is an indicator
that takes 1 if $k$ is 0 and 0 otherwise.

In plain English: the likelihood of our parameters given our data is the
probability that the site is occupied times the probability of the data
given that the site is occupied, plus the probability that the site
*isn't* occupied times the probability of the data given that it isn't
(which is 1 if all observations are 0, and 0 otherwise). We sum the two
probabilities because either one or the other is true, but not both.

We've now marginalized over $z$ by literally writing the values it could
take and the associated probabilities of the data, then summing across
them. Marginalizing isn't always this straightforward, because latent
states can take many (or infinitely many) values. We'll explore harder
contexts in future posts.

**You don't actually need to know any of that math for what follows**,
because we've coded it up for you.

# Implementing the occupancy model with nimbleEcology

```{r echo = F, include = F}
# I use the R package "AHMbook" to quickly simulate some occupancy data

set.seed(8706)
dat <- AHMbook::simOcc(M = 200, J = 3, 
                       mean.occupancy = 0.5, 
                       beta1 = 0.75, 
                       beta2 = -1,
                       beta3 = 0,
                       mean.detection = 0.4, 
                       alpha1 = 0, 
                       alpha2 = -0.5,
                       alpha3 = 0,
                       time.effects = c(0,0),
                       sd.lp = 0,
                       b = 0,
                       show.plot = FALSE)

obs <- dat$y

siteLevelData <- data.frame(
  elevation = dat$elev,
  forestCover = dat$forest
)

obsLevelData <- array(
  data = NA,
  dim = c(200, 3, 4),
  dimnames = list(NULL, NULL, c("elevation", "forestCover", "windLevel", "intercept"))
)

obsLevelData[, , "elevation"] <- 
  matrix(siteLevelData$elevation, nrow = 200, ncol = 3)
obsLevelData[, , "forestCover"] <- 
  matrix(siteLevelData$forest, nrow = 200, ncol = 3)
obsLevelData[, , "windLevel"] <- dat$wind

```

```{r swift, preview = TRUE, fig.cap="Swift (c) J Tanner 2022. <https://macaulaylibrary.org/asset/409111121>", echo = FALSE, out.width= "100%"}

knitr::include_graphics("images/swift.png")

```

<!-- ![Swift photo (c) J Tanner 2022. -->

<!-- <https://macaulaylibrary.org/asset/409111121>](images/swift.png) -->

In this section, I showcase the use of nimbleEcology for implementing an
occupancy model with simulated data.

Let's say we have some data we collected in a study on how the occupancy
of a bird, the white-throated swift. I have my detection/nondetection
data in a matrix called "obs". We surveyed 200 sites three times each,
and recorded whether or not we observed swifts during those surveys.

Here are the first six rows in the data:

```{r}
head(obs)
```

We detected the swift at three of the first six sites. Here are a few
more summary statistics:

```{r}
# Total number of sites with detections ("naive occupancy")
sum(rowSums(obs) > 0)

# Average number of observations per occupied site
mean(obs[rowSums(obs) > 0, ])
```

We want to know how the elevation and amount of forest cover impact the
occupancy of our swifts in the study area. We measured elevation and
forest cover at each of our sites. Here's the first six sites' data:

```{r}
head(siteLevelData)
```

(In the previous section, each row of these data are an $x_i$.)

We're using an occupancy model because we think our data contains false
negatives--i.e., sometimes we didn't see a swift at an occupied site. We
think that sometimes, high wind conditions could reduce our ability to
see swifts. And we also don't want to rule out the possibility that
detection probabilities vary with elevation or tree cover.
Disambiguating the effects of a single variable on **both** detection
and probability is one of the main benefits of th occupancy model.

Here's data from the first six sites for our three detection variables.

```{r}
head(obsLevelData[,,"elevation"])
head(obsLevelData[,,"forestCover"])
head(obsLevelData[,,"windLevel"])
```

For the sake of simplicity, we'll assume that our sites are random and
representative of the study area, that we have good reason to believe we
aren't recording false positive observations, and that we don't think
our detection process is variable except due to the factors we're
considering (wind, forest cover, and elevation).

Let's get to modeling!

```{r}
library(nimble)
library(nimbleEcology)
```

First, to use NIMBLE, we're going to define a `nimbleCode` object. This
code object uses a pseudo-code language, blending .

NIMBLE uses a declarative language, meaning order doesn't matter. A
consequence of this is that you can't overwrite values. Instead of
stepping through lines of code, think of `nimbleCode` as defining
relationships between variables (nodes). For a more in-depth overview of
the NIMBLE code language check out the NIMBLE User Manual, which I link
at the end of the post.

Ok, enough stalling. Here's the NIMBLE code for our model, with
comments.

```{r}

occuCode <- nimbleCode({
  # Loop over sites
  for (site in 1:nSites) {
    # Here's the nimbleEcology magic: the dOcc_* distribution.
    # This line of code says that our observations follow a probability
    #   distribution in the occupancy model, with a single occupancy probability
    #   and a vector of detection probabilities corresponding to each 
    #   observation. Note that we pass a whole site's worth of data at once.
    # Try running ?nimbleEcology::dOcc in your R session for more info.
    obs[site, 1:nReps] ~ dOcc_v(
      probOcc = psi[site], 
      probDetect = p[site, 1:nReps],
      len = nReps
    )
    
    # We define a logit-linear relationship between occupancy probability
    #   psi and some covariate data we'll provide.
    # inprod(x, y) is the same as x[1] * y[1] + x[2] * y[2] + ... + x[n] * y[n]
    #### Note that: ###
    # - We can put logit() on the left-hand side like a model equation
    # - The data intercept is represented in model matrix notation inside 
    #   occuCovars (more on that in a second)
    # - We need EXPLICIT INDEXING for our vectors (beta[1:nBeta], not just beta).
    #   This is an important difference between nimbleCode and base R
    logit(psi[site]) <- inprod(occuCovars[site, 1:nBeta], beta[1:nBeta])
    
    # Loop over reps
    for (rep in 1:nReps) {
      # Do this again for each detection probability. Only difference is that
      # p is now two-dimensional and detectionCovars is three-dimensional.
      logit(p[site, rep]) <- inprod(detectionCovars[site, rep, 1:nGamma],
                                    gamma[1:nGamma])
    } # END reps loop
  } # END site loop
  
  # Priors. We need these in order to do MCMC sampling. I'm going to choose some
  # arbitrary, mostly uninformative priors.
  # Priors on each covariate on occupancy:
  for (i in 1:nBeta) {
    beta[i] ~ dnorm(0, sd = 2.5)
  }
  # Priors on each covariate on detection:
  for (i in 1:nGamma) {
    gamma[i] ~ dnorm(0, sd = 2.5)
  }
})

```

Some of this code might look unfamiliar to folks who have done Bayesian
occupancy modeling before. I've gone out of my way to make things
generalizable, using `inprod()` over, for example,
`intercept + elev[i] * b1 + forest[i] * b2`. My motivation for doing
this is that it's super generalizable, and I'll be able to add or remove
covariates without making any changes to my code.

Note that because we're using nimbleEcology, we've "marginalized over"
the latent state $z$ and it's not in the model. This is a good thing for
a couple reasons: it reduces the amount of RAM needed, which can be
important for large datasets; it slightly speeds up mixing time in some
cases; and it saves some lines of code. However, there are often cases
where we **want** $z$, such as if we want a posterior distribution on
the number of occupied sites. In that case we shouldn't use
nimbleEcology (but NIMBLE will do fine).

The next step in the NIMBLE workflow is to define a NIMBLE model object.
This turns our code, which describes a series of nodes and the
relationships between them, into an actual object composed of that
structure.

All the data wrangling I have to do is just to get my inputs in the same
form as I've described them.

Because I'm using `inprod()` I need to make sure my data have intercept
columns first.

```{r}
siteLevelData$intercept <- 1
obsLevelData[,, "intercept"] <- 1
```

```{r}
occu_model <- nimbleModel(
  code = occuCode,
  constants = list(
    nBeta = 3,
    nGamma = 4,
    nSites = nrow(obs),
    nReps = ncol(obs)
  ),
  data = list(
    obs = obs,
    # I like indexing these here for two reasons: (1) I make sure they're going
    #   into the model in order, and (2) 
    detectionCovars = 
      obsLevelData[,, c("intercept", "elevation", "forestCover", "windLevel")],
    occuCovars = siteLevelData[, c("intercept", "elevation", "forestCover")]
  ),
  inits = list(
    beta = rnorm(3),
    gamma = rnorm(4)
  )
)
```

That's it! We now have a nimbleModel object.

One of my favorite things about working in NIMBLE is that we can play
with and query the nimbleModel.

```{r}
# Peek inside the model: what are the data values for the 22nd site?
occu_model$obs[22,]
# What's the corresponding site-level data at this site?
occu_model$occuCovars[22,] # Intercept, elevation, tree cover

# What's the log-likelihood of the model given initial values?
# (If this is non-NA, that means our model is initialized properly)
occu_model$calculate()

```

If we wanted to, we could modify data or initial values at this stage,
but not constants, which are "baked in" to the model when its built.

I'll go quickly over this next part, since using MCMC with NIMBLE is
better explained on the [NIMBLE examples
page](https://r-nimble.org/examples) (see: "Creating a default MCMC").
Briefly, we're going to build an MCMC object and then compile the whole
thing so we're ready to do some MCMC sampling.

```{r}
mcmc <- buildMCMC(occu_model)

# Compile to C++. This takes a second, but it's worth it!
complist <- compileNimble(occu_model, mcmc)

# We can still query / modify the compiled model
complist$occu_model$obs[22,]
```

Now let's do some MCMC sampling. 10,000 MCMC samples on 2 chains is a
bit overkill for this model, but it's quick--this takes about 20 seconds
to run on my machine.

```{r}
samples <- runMCMC(complist$mcmc, 
                   niter = 10000,
                   nburnin = 1000,
                   nchains = 2,
                   thin = 1,
                   samplesAsCodaMCMC = TRUE)
```

We can plot some of our chains to see how they mixed, for example the
logit-scale effect of elevation on occupancy, which is `beta[2]`
(because `beta[1]` is the intercept).

```{r}
plot(samples[, "beta[2]"])
```

I like the package `MCMCvis` for MCMC summaries.

```{r echo = FALSE}
options(digits = 2)

```

```{r}

summary <- MCMCvis::MCMCsummary(samples)

summary$param <- c("Intercept (occu)", "Elev (occu)", "Forest (occu)",
                   "Intercept (det)", "Elev (det)", "Forest (det)", "Wind (det)")

summary[, c("param", "mean", "2.5%", "97.5%")]
```

We found a negative effect of forest cover on occupancy, and a negative
effect of wind speed on detection. Nice.

# Next time...

Stay tuned for two follow up posts. In the first, I'll discuss easy ways
to extend the occupancy model in NIMBLE to showcase the flexibility of
this tool. Then, I'll showcase an N-mixture model, and we'll explore a
case where marginalizing over a latent state can dramatically improve
computation time.

Thanks for reading! Hit the "contact me" button at the top of the page
if you have any questions or feedback.

# Citations and additional reading

All the code in this exercise is available in the Github repository for
this blog, specifically here. Please feel free to download and play
around with this .Rmd file, and to copy and use this code freely.

If you want to see the code underlying the occupancy distribution used,
it's on [this GitHub
page](https://github.com/nimble-dev/nimbleEcology/blob/master/R/dOcc.R).
The nimbleEcology vignette is
[here](https://cran.r-project.org/web/packages/nimbleEcology/vignettes/Introduction_to_nimbleEcology.html).

For a really helpful blog post on implementing the occupancy model in
NIMBLE without marginalization, check out [Mason Fidino's writeup
here](https://masonfidino.com/a_simple_static_occupancy_model/). Dr.
Fidino's blog was a big inspiration in getting this one off the ground.

Relevant papers and books:

-   [Original MacKenzie et al. 2002
    paper](doi.org/10.1890/0012-9658(2002)083%5B2248:ESORWD%5D2.0.CO;2)
    on site-occupancy model
-   [Ponisio et al. 2020](doi.org/10.1002/ece3.6053) paper marginalizing
    distributions in NIMBLE
-   [Occupancy Estimation and Modeling
    textbook](https://www.sciencedirect.com/book/9780124071971/occupancy-estimation-and-modeling)
    by MacKenzie et al.

As always, check out the [NIMBLE User
Manual](https://r-nimble.org/manuals/NimbleUserManual.pdf) and the
[NIMBLE examples page](https://r-nimble.org/examples) for more NIMBLE
stuff.
